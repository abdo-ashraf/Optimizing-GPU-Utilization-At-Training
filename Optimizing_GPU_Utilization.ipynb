{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_steps = 20\n",
    "# Create an empty DataFrame to hold experments data\n",
    "results = pd.DataFrame(columns=[\"step\"])\n",
    "# fill step column data\n",
    "results[\"step\"] = range(number_of_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## flash-attention and mem_efficient-attention is enabled by default, so We wil disable it and re-enable it again at its section.\n",
    "torch.backends.cuda.enable_flash_sdp(enabled=False)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(enabled=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model that we will test on.\n",
    "\n",
    "The model we will be testing on is part of the NMT-MultiModel-Training-Framework project. You can find the implementation\n",
    "[NMT_Transformer](https://github.com/abdo-ashraf/NMT-MultiModel-Training-Framework/blob/main/Models/Transformer_model.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMT_Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size:int, dim_embed:int,\n",
    "                 dim_model:int, dim_feedforward:int, num_layers:int,\n",
    "                 dropout_probability:float, maxlen:int):\n",
    "        \"\"\"\n",
    "        Neural Machine Translation Transformer model.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Size of the vocabulary\n",
    "            dim_embed: Dimension of the embedding vectors\n",
    "            dim_model: Dimension of the model (hidden size)\n",
    "            dim_feedforward: Dimension of the feedforward network in transformer layers\n",
    "            num_layers: Number of encoder and decoder layers\n",
    "            dropout_probability: Dropout rate\n",
    "            maxlen: Maximum sequence length for positional embeddings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Shared embeddings for source and target tokens\n",
    "        self.embed_shared_src_trg_cls = nn.Embedding(num_embeddings=vocab_size, embedding_dim=dim_embed)\n",
    "        # Shared positional embeddings for source and target sequences\n",
    "        self.positonal_shared_src_trg = nn.Embedding(num_embeddings=maxlen, embedding_dim=dim_embed)\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(dropout_probability)\n",
    "\n",
    "        # Create encoder layer with specified parameters\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim_model, nhead=8,\n",
    "                                                   dim_feedforward=dim_feedforward,\n",
    "                                                   dropout=dropout_probability,\n",
    "                                                   batch_first=True, norm_first=True)\n",
    "        # Stack multiple encoder layers\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers, enable_nested_tensor=False)\n",
    "\n",
    "        # Create decoder layer with specified parameters\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=dim_model, nhead=8,\n",
    "                                                   dim_feedforward=dim_feedforward,\n",
    "                                                   dropout=dropout_probability,\n",
    "                                                   batch_first=True, norm_first=True)\n",
    "        # Stack multiple decoder layers\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Output projection layer to vocabulary size\n",
    "        self.classifier = nn.Linear(dim_model, vocab_size)\n",
    "        # Weight sharing between embedding and output projection (tied embeddings)\n",
    "        self.classifier.weight = self.embed_shared_src_trg_cls.weight\n",
    "\n",
    "        self.maxlen = maxlen\n",
    "        # Initialize model weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        Initialize the weights of the model using standard transformer initialization.\n",
    "        \n",
    "        Args:\n",
    "            module: Module to initialize\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, source, target, pad_tokenId):\n",
    "        \"\"\"\n",
    "        Forward pass of the NMT Transformer.\n",
    "        \n",
    "        Args:\n",
    "            source: Source sequence tensor [batch_size, source_seq_len]\n",
    "            target: Target sequence tensor [batch_size, target_seq_len]\n",
    "            pad_tokenId: Token ID used for padding\n",
    "            \n",
    "        Returns:\n",
    "            logits: Output logits [batch_size, target_seq_len, vocab_size]\n",
    "            loss: Cross-entropy loss if target length > 1, otherwise None\n",
    "        \"\"\"\n",
    "        # Get batch size and sequence lengths\n",
    "        B, Ts = source.shape\n",
    "        B, Tt = target.shape\n",
    "        device = source.device\n",
    "        \n",
    "        ## Encoder Path\n",
    "        # Create positional embeddings for source sequence\n",
    "        src_poses = self.positonal_shared_src_trg(torch.arange(0, Ts).to(device).unsqueeze(0).repeat(B, 1))\n",
    "        # Combine token embeddings with positional embeddings and apply dropout\n",
    "        src_embedings = self.dropout(self.embed_shared_src_trg_cls(source) + src_poses)\n",
    "\n",
    "        # Create padding mask for source sequence\n",
    "        src_pad_mask = source == pad_tokenId\n",
    "        # Pass through encoder to get memory\n",
    "        memory = self.transformer_encoder(src=src_embedings, mask=None, src_key_padding_mask=src_pad_mask, is_causal=False)\n",
    "        \n",
    "        ## Decoder Path\n",
    "        # Create positional embeddings for target sequence\n",
    "        trg_poses = self.positonal_shared_src_trg(torch.arange(0, Tt).to(device).unsqueeze(0).repeat(B, 1))\n",
    "        # Combine token embeddings with positional embeddings and apply dropout\n",
    "        trg_embedings = self.dropout(self.embed_shared_src_trg_cls(target) + trg_poses)\n",
    "        \n",
    "        # Create padding mask for target sequence\n",
    "        trg_pad_mask = target == pad_tokenId\n",
    "        # Create causal mask to prevent attending to future tokens\n",
    "        tgt_mask = torch.nn.Transformer.generate_square_subsequent_mask(Tt, dtype=bool).to(device)\n",
    "        # Pass through decoder\n",
    "        decoder_out = self.transformer_decoder.forward(tgt=trg_embedings,\n",
    "                                                memory=memory,\n",
    "                                                tgt_mask=tgt_mask,\n",
    "                                                memory_mask=None,\n",
    "                                                tgt_key_padding_mask=trg_pad_mask,\n",
    "                                                memory_key_padding_mask=None)\n",
    "        \n",
    "        ## Classifier Path\n",
    "        # Project decoder output to vocabulary space\n",
    "        logits = self.classifier(decoder_out)\n",
    "        \n",
    "        # Calculate loss if we have more than one target token\n",
    "        loss = None\n",
    "        if Tt > 1:\n",
    "            # For model logits we need all tokens except the last one\n",
    "            flat_logits = logits[:,:-1,:].reshape(-1, logits.size(-1))\n",
    "            # For targets we need all tokens except the first one (shift right)\n",
    "            flat_targets = target[:,1:].reshape(-1)\n",
    "            # Calculate cross-entropy loss, ignoring padding tokens\n",
    "            loss = nn.functional.cross_entropy(flat_logits, flat_targets, ignore_index=pad_tokenId)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "dim_embed = 512\n",
    "dim_model = 512\n",
    "dim_feedforward = 512*4\n",
    "num_layers = 3\n",
    "dropout_probability = 0.1\n",
    "maxlen = 50\n",
    "\n",
    "batch_size = 256 # reduce batch_size incase of insufficient memory error\n",
    "num_batch = 50\n",
    "data_x = torch.randint(0, 5000, (num_batch, batch_size, 50))\n",
    "data_y = torch.randint(0, 5000, (num_batch, batch_size, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 8.58, dt: 725.97ms\n",
      "step 1, loss: 8.65, dt: 487.70ms\n",
      "step 2, loss: 8.61, dt: 457.88ms\n",
      "step 3, loss: 8.60, dt: 473.04ms\n",
      "step 4, loss: 8.59, dt: 474.77ms\n",
      "step 5, loss: 8.60, dt: 475.53ms\n",
      "step 6, loss: 8.59, dt: 472.96ms\n",
      "step 7, loss: 8.59, dt: 475.89ms\n",
      "step 8, loss: 8.59, dt: 473.91ms\n",
      "step 9, loss: 8.57, dt: 472.66ms\n",
      "step 10, loss: 8.58, dt: 472.50ms\n",
      "step 11, loss: 8.58, dt: 474.64ms\n",
      "step 12, loss: 8.59, dt: 481.09ms\n",
      "step 13, loss: 8.58, dt: 479.27ms\n",
      "step 14, loss: 8.58, dt: 477.34ms\n",
      "step 15, loss: 8.57, dt: 478.87ms\n",
      "step 16, loss: 8.57, dt: 477.06ms\n",
      "step 17, loss: 8.56, dt: 476.01ms\n",
      "step 18, loss: 8.57, dt: 478.91ms\n",
      "step 19, loss: 8.58, dt: 481.44ms\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "model = NMT_Transformer(vocab_size=vocab_size,\n",
    "                    dim_embed=dim_embed,\n",
    "                    dim_model=dim_model,\n",
    "                    dim_feedforward=dim_feedforward,\n",
    "                    num_layers=num_layers,\n",
    "                    dropout_probability=dropout_probability,\n",
    "                    maxlen=maxlen)\n",
    "model.to('cuda')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "dts = []\n",
    "for i in range(number_of_steps):\n",
    "    t1 = time.time()\n",
    "    x = data_x[i].to('cuda')\n",
    "    y = data_y[i].to('cuda')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x, y, 0)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    t2 = time.time()\n",
    "    dt = (t2 - t1)*1000\n",
    "    dts.append(round(dt, 2))\n",
    "    print(f\"step {i}, loss: {loss.item():.2f}, dt: {dt:.2f}ms\")\n",
    "results[\"no_optimization\"] = dts\n",
    "print(logits.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](download.png)\n",
    "\n",
    "[NVIDIA Ampere Architecture Whitepaper](https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Data Types: From Float32 to TensorFloat32 (TF32)\n",
    "TensorFloat32 (TF32) is a computational format designed to accelerate matrix multiplications while maintaining numerical stability. It is particularly useful for deep learning workloads on NVIDIA GPUs.\n",
    "\n",
    "### Understanding TF32 in Matrix Multiplications\n",
    "\n",
    "The input operand refers to the numbers being multiplied in matrix multiplications.\n",
    "\n",
    "TF32 Input Operand:\n",
    "- Uses 10-bit mantissa (instead of 23-bit in FP32).\n",
    "\n",
    "The accumulator is where the results of multiplications are summed up during matrix multiplications (e.g., in matrix-matrix multiplications).\n",
    "TF32 Accumulator:\n",
    "- Although the inputs have lower precision (10-bit mantissa), the accumulation happens in full FP32 (23-bit mantissa).\n",
    "\n",
    "### TF32 is Not a Data Type\n",
    "- Unlike FP32, FP16, or INT8, TF32 is not a format that can be explicitly stored in memory.\n",
    "- Instead, TF32 computations are performed at the hardware level, but all stored values remain in FP32 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 8.57, dt: 485.42ms\n",
      "step 1, loss: 8.63, dt: 375.30ms\n",
      "step 2, loss: 8.61, dt: 377.14ms\n",
      "step 3, loss: 8.60, dt: 375.05ms\n",
      "step 4, loss: 8.61, dt: 376.04ms\n",
      "step 5, loss: 8.59, dt: 380.01ms\n",
      "step 6, loss: 8.60, dt: 381.30ms\n",
      "step 7, loss: 8.58, dt: 382.51ms\n",
      "step 8, loss: 8.59, dt: 375.97ms\n",
      "step 9, loss: 8.57, dt: 377.80ms\n",
      "step 10, loss: 8.59, dt: 381.28ms\n",
      "step 11, loss: 8.58, dt: 385.90ms\n",
      "step 12, loss: 8.58, dt: 388.09ms\n",
      "step 13, loss: 8.57, dt: 385.71ms\n",
      "step 14, loss: 8.57, dt: 387.24ms\n",
      "step 15, loss: 8.57, dt: 385.37ms\n",
      "step 16, loss: 8.57, dt: 386.28ms\n",
      "step 17, loss: 8.57, dt: 386.79ms\n",
      "step 18, loss: 8.57, dt: 388.25ms\n",
      "step 19, loss: 8.57, dt: 386.14ms\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "###################-Changes-######################\n",
    "torch.set_float32_matmul_precision('high')\n",
    "## highest -> FP32\n",
    "## high -> TP32\n",
    "## medium -> BF16 not recommended (for BF use torch.autocast)\n",
    "##################################################\n",
    "\n",
    "model = NMT_Transformer(vocab_size=vocab_size,\n",
    "                    dim_embed=dim_embed,\n",
    "                    dim_model=dim_model,\n",
    "                    dim_feedforward=dim_feedforward,\n",
    "                    num_layers=num_layers,\n",
    "                    dropout_probability=dropout_probability,\n",
    "                    maxlen=maxlen)\n",
    "model.to('cuda')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters() ,lr=3e-4)\n",
    "\n",
    "dts = []\n",
    "for i in range(number_of_steps):\n",
    "    t1 = time.time()\n",
    "    x = data_x[i].to('cuda')\n",
    "    y = data_y[i].to('cuda')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x, y, 0)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    t2 = time.time()\n",
    "    dt = (t2 - t1)*1000\n",
    "    dts.append(round(dt, 2))\n",
    "    print(f\"step {i}, loss: {loss.item():.2f}, dt: {dt:.2f}ms\")\n",
    "results[\"TF32\"] = dts\n",
    "print(logits.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As you may observe that logits type is still float32, but this is ok as TensorFloat32 is not a storage format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Data Types to BrainFloat16 (BF16)\n",
    "\n",
    "BrainFloat16 (BF16) is a lower-precision floating-point format designed to improve computational efficiency while maintaining numerical stability in deep learning tasks.\n",
    "\n",
    "### BF16 vs. FP16\n",
    " - You can reduce the precision to Float16 (FP16) for performance gains, but FP16 has a lower mantissa precision compared to BF16.\n",
    " - When using FP16, you need to apply a gradient scaler (e.g., torch.amp.GradScaler) to prevent numerical instability during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 8.58, dt: 1124.07ms\n",
      "step 1, loss: 8.63, dt: 221.94ms\n",
      "step 2, loss: 8.62, dt: 222.62ms\n",
      "step 3, loss: 8.60, dt: 221.68ms\n",
      "step 4, loss: 8.61, dt: 222.46ms\n",
      "step 5, loss: 8.58, dt: 747.42ms\n",
      "step 6, loss: 8.60, dt: 225.76ms\n",
      "step 7, loss: 8.59, dt: 227.54ms\n",
      "step 8, loss: 8.59, dt: 225.77ms\n",
      "step 9, loss: 8.57, dt: 226.55ms\n",
      "step 10, loss: 8.58, dt: 224.77ms\n",
      "step 11, loss: 8.58, dt: 222.19ms\n",
      "step 12, loss: 8.58, dt: 226.65ms\n",
      "step 13, loss: 8.59, dt: 231.82ms\n",
      "step 14, loss: 8.58, dt: 224.42ms\n",
      "step 15, loss: 8.58, dt: 224.67ms\n",
      "step 16, loss: 8.58, dt: 221.99ms\n",
      "step 17, loss: 8.58, dt: 221.90ms\n",
      "step 18, loss: 8.57, dt: 222.73ms\n",
      "step 19, loss: 8.57, dt: 227.17ms\n",
      "torch.cuda.BFloat16Tensor\n"
     ]
    }
   ],
   "source": [
    "model = NMT_Transformer(vocab_size=vocab_size,\n",
    "                    dim_embed=dim_embed,\n",
    "                    dim_model=dim_model,\n",
    "                    dim_feedforward=dim_feedforward,\n",
    "                    num_layers=num_layers,\n",
    "                    dropout_probability=dropout_probability,\n",
    "                    maxlen=maxlen)\n",
    "model.to('cuda')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters() ,lr=3e-4)\n",
    "\n",
    "dts = []\n",
    "for i in range(number_of_steps):\n",
    "    t1 = time.time()\n",
    "    x = data_x[i].to('cuda')\n",
    "    y = data_y[i].to('cuda')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    ###################-Changes-######################\n",
    "    ## only do model forward and loss calculations inside autocast context\n",
    "    ## to use torch.float16 you will need to use gradscaler\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y, 0)\n",
    "    ##################################################\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    t2 = time.time()\n",
    "    dt = (t2 - t1)*1000\n",
    "    dts.append(round(dt, 2))\n",
    "    print(f\"step {i}, loss: {loss.item():.2f}, dt: {dt:.2f}ms\")\n",
    "results[\"BF16\"] = dts\n",
    "print(logits.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Form now, we will continue using the BF16 data type while stacking other optimizations.\n",
    "- Some of the upcoming optimizations may not work on older GPUs like NVIDIA P100 and T4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.compile – **A Game Changer**\n",
    "\n",
    "torch.compile brings significant performance improvements by optimizing PyTorch models at runtime. It primarily achieves this through two key optimizations:\n",
    "1. Reducing Python Overhead:\n",
    "    - torch.compile compiles the entire model (or function), allowing PyTorch to analyze and optimize the execution as if it were written in C++.\n",
    "    - This minimizes Python’s dynamic execution overhead and enables more efficient computation.\n",
    "\n",
    "2. Reducing GPU Read/Writes:\n",
    "    - Since PyTorch now has a global view of your model, it can apply kernel fusion to optimize operations.\n",
    "    - Kernel fusion reduces redundant GPU memory accesses, improving efficiency and execution speed.\n",
    "\n",
    "To better understand kernel fusion, you can watch this explanation: [Kernel Fusion explanation video](https://www.youtube.com/watch?v=PBdpNhaBxfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0302 17:54:28.141000 5443 torch/_inductor/utils.py:1137] [0/0_1] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 8.58, dt: 108770.06ms\n",
      "step 1, loss: 8.64, dt: 1376.54ms\n",
      "step 2, loss: 8.61, dt: 175.19ms\n",
      "step 3, loss: 8.61, dt: 168.29ms\n",
      "step 4, loss: 8.58, dt: 169.81ms\n",
      "step 5, loss: 8.59, dt: 172.43ms\n",
      "step 6, loss: 8.59, dt: 172.27ms\n",
      "step 7, loss: 8.58, dt: 169.86ms\n",
      "step 8, loss: 8.59, dt: 171.13ms\n",
      "step 9, loss: 8.59, dt: 170.33ms\n",
      "step 10, loss: 8.58, dt: 169.26ms\n",
      "step 11, loss: 8.58, dt: 171.12ms\n",
      "step 12, loss: 8.58, dt: 170.53ms\n",
      "step 13, loss: 8.58, dt: 170.80ms\n",
      "step 14, loss: 8.56, dt: 174.60ms\n",
      "step 15, loss: 8.57, dt: 184.15ms\n",
      "step 16, loss: 8.56, dt: 190.05ms\n",
      "step 17, loss: 8.57, dt: 189.92ms\n",
      "step 18, loss: 8.57, dt: 186.96ms\n",
      "step 19, loss: 8.58, dt: 185.58ms\n",
      "torch.cuda.BFloat16Tensor\n"
     ]
    }
   ],
   "source": [
    "model = NMT_Transformer(vocab_size=vocab_size,\n",
    "                    dim_embed=dim_embed,\n",
    "                    dim_model=dim_model,\n",
    "                    dim_feedforward=dim_feedforward,\n",
    "                    num_layers=num_layers,\n",
    "                    dropout_probability=dropout_probability,\n",
    "                    maxlen=maxlen)\n",
    "model.to('cuda')\n",
    "###################-Changes-######################\n",
    "model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "## mode specifies what the compiler should be optimizing while compiling.\n",
    "##      \"default\": compile efficiently without taking too long to compile or using extra memory.\n",
    "##      \"reduce-overhead\": reduce the framework overhead by a lot more, but cost a small amount of extra memory.\n",
    "##      \"max-autotune\": compiles for a long time, trying to give you the fastest code it can generate.\n",
    "##################################################\n",
    "optimizer = torch.optim.AdamW(model.parameters() ,lr=3e-4)\n",
    "\n",
    "dts = []\n",
    "for i in range(number_of_steps):\n",
    "    t1 = time.time()\n",
    "    x = data_x[i].to('cuda')\n",
    "    y = data_y[i].to('cuda')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    ###################-Changes-######################\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y, 0)\n",
    "    ##################################################\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    t2 = time.time()\n",
    "    dt = (t2 - t1)*1000\n",
    "    dts.append(round(dt, 2))\n",
    "    print(f\"step {i}, loss: {loss.item():.2f}, dt: {dt:.2f}ms\")\n",
    "results[\"BF16+TC\"] = dts\n",
    "print(logits.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FlashAttention\n",
    "[FlashAttention Paper](https://arxiv.org/abs/2205.14135)\n",
    "\n",
    "FlashAttention is a specialized kernel fusion optimization designed to improve the efficiency of attention mechanisms in deep learning models. Unlike torch.compile, which applies general optimizations, FlashAttention specifically targets self-attention computations.\n",
    "\n",
    "### How FlashAttention Works\n",
    "\n",
    "- The authors of FlashAttention rearranged the causal attention algorithm to minimize memory overhead.\n",
    "- They used online softmax approach to efficiently perform kernel fusion [Online softmax Paper](https://arxiv.org/abs/1805.02867).\n",
    "- This reduces redundant memory reads/writes, significantly improving training speed and memory efficiency.\n",
    "\n",
    "### Performance Gains\n",
    "- FlashAttention performs more FLOPs than regular attention but achieves a 7.6× speedup in attention computation.\n",
    "- The key reason behind this acceleration is the reduction in memory access times, the algorithm never explicitly stores (materializes) the attention matrix, eliminating costly memory operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provides different optimized implementations of Scaled Dot Product Attention (SDPA), including FlashAttention, Memory-Efficient SDPA, and Math-based SDPA. These implementations vary in speed and memory efficiency based on hardware availability.\n",
    "\n",
    "1. FlashAttention (flash_sdp)\n",
    "    - `torch.backends.cuda.enable_flash_sdp()`: Globally enables or disables FlashAttention.\n",
    "    - Highly optimized for NVIDIA GPUs with Ampere (RTX 30xx, A100) or newer architectures.\n",
    "    - Typically the fastest option if supported.\n",
    "\n",
    "2. Memory-Efficient Attention (mem_efficient_sdp)\n",
    "    - `torch.backends.cuda.enable_mem_efficient_sdp()`: Globally enables or disables Memory-Efficient Attention.\n",
    "    - Works across more hardware than FlashAttention.\n",
    "    - Often provides performance benefits similar to FlashAttention but with slightly higher memory usage.\n",
    "\n",
    "3. Math-based Attention (math_sdp)\n",
    "    - `torch.backends.cuda.enable_math_sdp()`: Globally enables or disables the PyTorch C++ implementation.\n",
    "    - A fallback, purely mathematical implementation.\n",
    "    - Usually the slowest and most memory-intensive.\n",
    "    - Used when no hardware-optimized SDPA is available.\n",
    "\n",
    "(GPT-4o generated)\n",
    "\n",
    "### Automatic Selection in PyTorch\n",
    "\n",
    "By default, PyTorch automatically selects the best available SDPA implementation when you use `torch.nn.functional.scaled_dot_product_attention(query, key, value)` API or Transformer layer (As we did in this notebook):\n",
    "1. flash_sdp (if supported)\n",
    "2. mem_efficient_sdp (if flash_sdp is disabled or unsupported)\n",
    "3. math_sdp (last fallback option)\n",
    "\n",
    "For more details, refer to the official PyTorch documentation:\n",
    "[PyTorch Documentation on scaled_dot_product_attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flash Attention Enabled: False\n",
      "Memory Efficient Attention Enabled: False\n",
      "Math Attention Enabled: True\n"
     ]
    }
   ],
   "source": [
    "## to Check Available Options:\n",
    "print(\"Flash Attention Enabled:\", torch.backends.cuda.flash_sdp_enabled())\n",
    "print(\"Memory Efficient Attention Enabled:\", torch.backends.cuda.mem_efficient_sdp_enabled())\n",
    "print(\"Math Attention Enabled:\", torch.backends.cuda.math_sdp_enabled())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 8.57, dt: 1370.61ms\n",
      "step 1, loss: 8.63, dt: 190.48ms\n",
      "step 2, loss: 8.63, dt: 183.07ms\n",
      "step 3, loss: 8.59, dt: 187.05ms\n",
      "step 4, loss: 8.60, dt: 181.42ms\n",
      "step 5, loss: 8.60, dt: 180.87ms\n",
      "step 6, loss: 8.59, dt: 180.70ms\n",
      "step 7, loss: 8.59, dt: 180.75ms\n",
      "step 8, loss: 8.57, dt: 183.63ms\n",
      "step 9, loss: 8.58, dt: 180.67ms\n",
      "step 10, loss: 8.58, dt: 181.14ms\n",
      "step 11, loss: 8.58, dt: 179.58ms\n",
      "step 12, loss: 8.57, dt: 183.46ms\n",
      "step 13, loss: 8.58, dt: 183.76ms\n",
      "step 14, loss: 8.58, dt: 182.51ms\n",
      "step 15, loss: 8.57, dt: 182.83ms\n",
      "step 16, loss: 8.56, dt: 182.23ms\n",
      "step 17, loss: 8.57, dt: 182.84ms\n",
      "step 18, loss: 8.57, dt: 183.75ms\n",
      "step 19, loss: 8.57, dt: 181.55ms\n",
      "torch.cuda.BFloat16Tensor\n"
     ]
    }
   ],
   "source": [
    "###################-Changes-######################\n",
    "## Re-enable flash and mem_efficient attentions and .\n",
    "torch.backends.cuda.enable_flash_sdp(enabled=True)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(enabled=True)\n",
    "##################################################\n",
    "\n",
    "model = NMT_Transformer(vocab_size=vocab_size,\n",
    "                    dim_embed=dim_embed,\n",
    "                    dim_model=dim_model,\n",
    "                    dim_feedforward=dim_feedforward,\n",
    "                    num_layers=num_layers,\n",
    "                    dropout_probability=dropout_probability,\n",
    "                    maxlen=maxlen)\n",
    "model.to('cuda')\n",
    "###################-Changes-######################\n",
    "model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "##################################################\n",
    "optimizer = torch.optim.AdamW(model.parameters() ,lr=3e-4)\n",
    "\n",
    "dts = []\n",
    "for i in range(number_of_steps):\n",
    "    t1 = time.time()\n",
    "    x = data_x[i].to('cuda')\n",
    "    y = data_y[i].to('cuda')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    ###################-Changes-######################\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y, 0)\n",
    "    ##################################################\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    t2 = time.time()\n",
    "    dt = (t2 - t1)*1000\n",
    "    dts.append(round(dt, 2))\n",
    "    print(f\"step {i}, loss: {loss.item():.2f}, dt: {dt:.2f}ms\")\n",
    "results[\"BF16+TC+FA\"] = dts\n",
    "print(logits.type())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fused optimizer\n",
    "In PyTorch's AdamW optimizer, the `fused` and `foreach` parameters control how the optimization step is performed under the hood. Here’s the difference between them:\n",
    "1. `fused` (Default: `None`):\n",
    "- When `fused=True`, PyTorch uses a fused CUDA kernel for optimization updates.\n",
    "- This can lead to faster training on GPUs by reducing memory operations and kernel launch overhead.\n",
    "- Requires the model's parameters to be on the GPU.\n",
    "- If `fused=True` but fusion is not supported for some reason (e.g., non-GPU tensors), PyTorch will fall back to the standard implementation.\n",
    "\n",
    "2. `foreach` (Default: `None`):\n",
    "- When `foreach=True`, PyTorch applies updates in a batched way instead of looping through each parameter individually.\n",
    "- This is beneficial for both CPU and GPU training, offering speed improvements by reducing Python overhead.\n",
    "- If `foreach=None`, PyTorch automatically decides whether to use foreach-based updates.\n",
    "\n",
    "Note: `fused` and `foreach` cannot be `True` together.\n",
    "\n",
    "You can learn more at [PyTorch Documentation on AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 8.57, dt: 1822.96ms\n",
      "step 1, loss: 8.66, dt: 206.46ms\n",
      "step 2, loss: 8.61, dt: 203.35ms\n",
      "step 3, loss: 8.60, dt: 207.05ms\n",
      "step 4, loss: 8.59, dt: 205.07ms\n",
      "step 5, loss: 8.60, dt: 201.60ms\n",
      "step 6, loss: 8.59, dt: 204.53ms\n",
      "step 7, loss: 8.58, dt: 202.52ms\n",
      "step 8, loss: 8.59, dt: 203.50ms\n",
      "step 9, loss: 8.58, dt: 203.43ms\n",
      "step 10, loss: 8.58, dt: 201.87ms\n",
      "step 11, loss: 8.58, dt: 203.68ms\n",
      "step 12, loss: 8.58, dt: 202.92ms\n",
      "step 13, loss: 8.59, dt: 205.46ms\n",
      "step 14, loss: 8.58, dt: 204.77ms\n",
      "step 15, loss: 8.57, dt: 203.57ms\n",
      "step 16, loss: 8.57, dt: 202.92ms\n",
      "step 17, loss: 8.58, dt: 201.67ms\n",
      "step 18, loss: 8.57, dt: 203.14ms\n",
      "step 19, loss: 8.57, dt: 203.65ms\n",
      "torch.cuda.BFloat16Tensor\n"
     ]
    }
   ],
   "source": [
    "model = NMT_Transformer(vocab_size=vocab_size,\n",
    "                    dim_embed=dim_embed,\n",
    "                    dim_model=dim_model,\n",
    "                    dim_feedforward=dim_feedforward,\n",
    "                    num_layers=num_layers,\n",
    "                    dropout_probability=dropout_probability,\n",
    "                    maxlen=maxlen)\n",
    "model.to('cuda')\n",
    "###################-Changes-######################\n",
    "model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "##################################################\n",
    "###################-Changes-######################\n",
    "optimizer = torch.optim.AdamW(model.parameters() ,lr=3e-4, fused=True)\n",
    "## fused and foreach cannot be True together.\n",
    "##################################################\n",
    "\n",
    "dts = []\n",
    "for i in range(number_of_steps):\n",
    "    t1 = time.time()\n",
    "    x = data_x[i].to('cuda')\n",
    "    y = data_y[i].to('cuda')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    ###################-Changes-######################\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y, 0)\n",
    "    ##################################################\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    t2 = time.time()\n",
    "    dt = (t2 - t1)*1000\n",
    "    dts.append(round(dt, 2))\n",
    "    print(f\"step {i}, loss: {loss.item():.2f}, dt: {dt:.2f}ms\")\n",
    "results[\"BF16+TC+FA+FuO\"] = dts\n",
    "print(logits.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## others\n",
    "1. Gradiant clipping\n",
    "2. LR Scheduler\n",
    "3. Nice numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results for ploting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"./results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [NVIDIA Ampere Architecture Whitepaper](https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf)\n",
    "- [PyTorch Documentation on set_float32_matmul_precision](https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html)\n",
    "- [PyTorch Documentation on Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html)\n",
    "- [PyTorch Documentation on torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)\n",
    "- [PyTorch Documentation on scaled_dot_product_attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\n",
    "- [FlashAttention Paper](https://arxiv.org/abs/2205.14135)\n",
    "- [Online softmax Paper](https://arxiv.org/abs/1805.02867)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
